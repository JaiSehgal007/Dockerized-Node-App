Problem statement:
As a developer I am having a local machine, it have some hardware and sw config. ( Win 11, Intel , Node, Redis etc.)
and there is a nodejs server which is running fine
The problem occur when there is a team  when a new developer comes, it must install the same version to run the code when copied from github

1st problem : an extra effor to set up the whole env
2nd : version vary ( cause error) this leads to change in environment
3rd : even if versions changes, the OS incompatibility starts to trouble
4th : If deployed on cloud, there also we need to set up the same configuration, and if we are in auto 
      scaling then we need to this repetitive task in all the machines


so in crux  we can say docker solves the problem to replicate environment


So in docker we make containers, in these containers we do our configurations, what would be its OS, what would be its tools and all such factors
And thses containers are shared with tht eteam or Cloud platform
Now it doesn't matter what os or any other config we have we can run that container on our system
These containers are light weight, we can build and destroy these containers anytime

Now intalling docker 

we have two things here
 
Docker Daemon: It is a tool, which is the actula docker, which spin up, build and scale down the containers
Docker Desktop: It is a GUI, which shows what is the actual state of our machine

in terminal:

docker -v ->version
docker run -it ubuntu -> run ubuntu container in interactive mode

( if docker daemon not running run command : "C:\Program Files\Docker\Docker\DockerCli.exe" -SwitchDaemon)

we can see that in my machine I am having a ubuntu container, and in the containers section we can see ubuntu container running

now in the terminal we are in the ubuntu container and any command we run there is inside the container

so when we run command what happen is, if the image is available, so it reuse it and make a new container 
or else it download it from hub.docker.com (it is like github for containers)

we can do ctrl+d to exit


What are container and images:
Now like I am having dell g15 laptop and the os inside it is windows
so we can say that this os is working on this machine
similarly our images are like an os  and to run those images we need a container

container can't do anything on their own, they are just like a machine

and let say we are having 3 containers, so we can run the same image on all the three containers
and these containers are isolated

so when we work in development we make a custom image -> (like an image having ubuntu, node, mongoDB)
now we can give any name to this image and we can ask the developers to run this image in a container
and the config which is in this image is my own, this image can be published on the dockerhub
and thefellow developers can run this in their local containers and similary it can used in cloud also

docker container ls -> lists the running containers
so if we are working with cloud, we just have the terminal and not the docker desktop GUI
docker container ls -a -> it show all the containers

docker start <container-name> -> starts a stopped container
docker stop <name> -> stops the container

docker run --rm my_python_app:latest (rm flag to delete the contained after execution)

docker exec <name> ls -> this runs the container, performs the command and the gives result and stops the container

docker exec -it <name> bash -> this way the container do not stop after executing command

docker images / docker image ls  -> shows the available images

so this means that all the comapnies have thier docke images on the dockerhub and we can self host those images in your local container


docker run: The docker run command is used to create and start a new Docker container from an image.
docker start: The docker start command is used to start one or more existing stopped containers.
docker exec: The docker exec command is used to run a command inside a running container.

so how does these containers work internally?

here comes the concept of port mapping

now lets say we run a nodejs application in a container
now it is a server, and to run a server we require a port, now the port lets say which is 8000, is inside the container
and the containers are isolated, and in my local machine i wont be able to access it, to do so we need to expose the port

now lets say we run a mailhog server (an image using which we can send mails)
docker run -it mailhog/mailhog
this runs on port 1025
now to expose its port we can do the following

docker run -it -p 1020:1025 mailhog/mailhog

it runs the 1025 port of this container inside the port 1020 of my local machine 

Now how to pass the env variables inside my docker container?
docker run -it -p 8000:8000 -e key=value -e key2=value2 node
this way we are passing the env variables in our docker container


How to dockerize an Application?

now that we have created a basic nodejs application which is having some Files
now we want to make a image of these files, so that it can be run on other containers easily

for that we create a Dockerfile, in which we write all the configuration of how to make the image

for this we first select the base image

FROM ubuntu -> to use ubuntu as the base image
so this makes a base layer of ubuntu in the container

now on this ubuntu we have to install nodejs

(search web on how to install node on ubuntu)
for that we need to do apt-get update

RUN apt-get update

then installing curl

RUN apt-get install -y curl

(-y means yes to all questions asked in installation)


after running all the setup do:

COPY package.json package.json

where it says COPY source destination

source is package.json which get capoied inside the container

then we specify the entrypoint

ENTRYPOINT [ "node", "index.js" ]

i.e whenever this images is run in a container we run node index.js

the run 
docker build -t dockerized-node-app .
'.' means current directory
this will bulid the image

now what is caching the image?
in case we do not change anything and again run build
in this case it notices that there is nothing changed so it all gets done very fast
not that the first lne after which we get the change, all the lower lines will be ececuted again
so it is a good practise to install all the thing before and then copy (basically keep things structures wisely)

this is also called layer caching (i.e it executes the code below the changed line)

to publish this image to the hub, go to hub.docker.com
and create a account, and then create repository
give the name to your image and create an image
this creates a empty image
now with the name given by the hub 'username/imagename', create an image with this name locally

now enter command
docker push <image_name>

(ensure you do docker login first to start it)

now if the image is public then anyone can use it, we just need to enter the environment variables and do the port mapping


What is Docker Compose?
Lets say we are open souce contributer and we want to contribute to a website
so we need to run multiple containers 
1st container of postgres with port 5432
2nd container of redis  on port 6372
the mailhog and all others

so the first way is to execute these commands one by one and then run them
or else a best way is to use docker Compose

using it we can setup/create/destroy multiple containers

so to do this we use create a file docker-compose.yml

so to use multiple containers, we can give the whole configuration in it

version: '3.8' this is the version og the docker compose

services:
   my_postgres_sevice:
      image: postgres # for this service we need image from hub.docker.com
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_DB: review
      POSTGRES_PASSWORD: password

  redis:
    image: redis
    ports:
      - "6379:6379"

then we need to write following in terminal:

docker compose up

this will make all the configurations up and running

now in the docker desktop we can see in the container their is a stack running anmed 'docker' which is the name of the parent folder
and in this there is redis and postgres running

docker compose down -> destroys everything
docker composeup -d -> this means in detached mode (in the background)


to use this service in some another image we can use the following compose file

version: '3.8'

services:
  my_postgres_service:
    image: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_DB: review
      POSTGRES_PASSWORD: password

  redis:
    image: redis
    ports:
      - "6379:6379"

  node_app:
    build:
      context: ./path/to/your/node/app
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    depends_on:
      - my_postgres_service
      - redis

What is Docker Networking?

spin one container first
docker run -it --name <container_name> <image_name>

now in this container if we write ping google.com
then we can see that it can ping google, i.e it can communicate with the outer world internet
this is what docker networking is

Search for docker networking, there we can see various drives using which our container can talk to the internet

1st driver -> bridge mode network -> default driver (when we specify nothing), 
i.e if we do not specify network while creating container it by default connet to the bridge network

command: docker network inspect bridge

this give the list of containers that are connected to this network and running

so bridge is -> the network establishment between the host machine and the docker container
this is how ot get internet connectivity

docker network ls -> gives the list of the drivers
we can custom create a network also -> User-defined bridge networks enable containers on the same Docker host to communicate with each other


Second driver is host :

command docker run -it --network=host busybox

now this container is in the host mode, this means it has not got the bridge, it is directly
connected with the host machine's network

so what is the major difference in between them in reality?
we studied port mapping earlier, we exposed the ports as they were inside docker
but if the networking is host mode, then we need not give the port mapping as they are on same network

the last one is none -> such container doesn't have any access to the internet

Now how to make custom network using docker?

command -> docker network create -d bridge <network_name>

we first gave our network a driver, 'bridge', means we want to make the network in bridge mode

docker network create -d bridge youtube

now when we run a container with command

docker run -it --network=youtube --name tony_stark ubuntu

now creating one more container 

docker run -it --network=youtube --name doc_strange busybox

now  we have 2 different containers, on the same network, and both have different os

now as they both are on the same network, they both can communicate

to check:
ping tony_stark

it we stop one container the connection automatically breaks

docker inspect youtube -> will give all the containers connected to this network with thier ip address
so we can ping using ip or using name

so when we work in docker compose, we make our cutom network, in which we can run the database and the realted apllicaitons on the same network

then we need not manage the ip addresses, as these ip address keeps on changing


What is Volume Mounting?
when ever we make a container, it have some memory, and when it destroys, then the moeory also gets destryoed

to prevent this we are having the option of 'docker volumes', we can mount volumes inside of docker
which act as a permanent storage

lets say we are having a container of ubuntu and a host windows
but we can mount a particular directory/volume over ubuntu

we will mount C:\docker_folders to the unbuntu container
so this container will have the access to only this directory of my host

command: docker run -it -v C:\docker_folders:/home/abc

this mount theis volume of host to home/abc folder of the container
so even this conainer gets destroyed then also the data is preserved

we can even create our own volumes (custom volumes)

command: docker volume create <volume_name>

now to mount this we just have to give command while creating container
--mount source=<volume_name>

this also help prevent losing of volumes


What is efficient Caching in layers?

now we have layes in the docker file:

Layer1:
FROM ubuntu

Layer2:
RUN apt-get update
RUN apt-get install -y curl
RUN curl -sL https://deb.nodesource.com/setup_18.x | bash -
RUN apt-get upgrade -y
RUN apt-get install -y nodejs

Layer3:
COPY package.json package.json
COPY package-lock.json package-lock.json
COPY .gitignore .gitignore
COPY index.js index.js

Layer4:
RUN npm install

Layer5:
ENTRYPOINT [ "node", "index.js" ]


by default these layers are cached
so if we cahnge the code for one layer then it changes the code for all the subsequent layers

one efficiency we can make is:

Layer3:
COPY package.json package.json
COPY package-lock.json package-lock.json

Layer4:
RUN npm install

Layer5:
COPY .gitignore .gitignore
COPY index.js index.js

so that npm install is only done when there is change is dependecies

another change we can make is that,when we have lots of files, we need not copy all of them
we just need to change

Layer5:
COPY .gitignore .gitignore
COPY index.js index.js

to

Layer5:
COPY . .

that is pick everythin from this package and paste it in '.' of the container

now to remove the node_modules and othe such king of files we have to create a .dockerignore file



now we have the docker file as

FROM ubuntu

RUN apt-get update
RUN apt-get install -y curl
RUN curl -sL https://deb.nodesource.com/setup_18.x | bash -
RUN apt-get upgrade -y
RUN apt-get install -y nodejs

COPY package.json /app/package.json
COPY package-lock.json /app/package-lock.json

RUN cd app && npm install

COPY .gitignore /app/.gitignore
COPY index.js /app/index.js

ENTRYPOINT [ "node", "app/index.js" ]

now here also we need to put /app again and again, to remove this we need to write

WORKDIR /app

this will inform docker to run all the code below this line to run inside the /app folder
so the updated docker file looks like

FROM ubuntu

RUN apt-get update
RUN apt-get install -y curl
RUN curl -sL https://deb.nodesource.com/setup_18.x | bash -
RUN apt-get upgrade -y
RUN apt-get install -y nodejs

WORKDIR /app

COPY package.json package.json
COPY package-lock.json package-lock.json

RUN npm install

COPY .gitignore .gitignore
COPY index.js index.js

ENTRYPOINT [ "node", "index.js" ]



What is docker Multi Stage Builds?
till now whatt we have done is single stage build, as we just picked up a base image and build
the way of working of multistage build is:

Now lets say i am having a typescript code, I have to compile that in simple js to run
now if I am asked to write its pipeline 

the our pipeline would be like

FROM ubuntu

RUN apt-get update
RUN apt-get install -y curl
RUN curl -sL https://deb.nodesource.com/setup_18.x | bash -
RUN apt-get upgrade -y
RUN apt-get install -y nodejs
RUN apt-get install typescript

COPY package.json /app/package.json
COPY package-lock.json /app/package-lock.json

RUN npm install
RUN tsc -p . # for building

COPY .gitignore /app/.gitignore
COPY index.js /app/index.js

ENTRYPOINT [ "node", "app/index.js" ]

this increases the size of my container, we can see that we do not need the typescript in production
as when my code is compiled then we do not need typescript, we just need it while it is building

to resolve this kind of thing we use multistage Builds


FROM ubuntu as build 
# this makes it as a build step, which do not have any entry point
# and we just do the work of building over here

RUN apt-get update
RUN apt-get install -y curl
RUN curl -sL https://deb.nodesource.com/setup_18.x | bash -
RUN apt-get upgrade -y
RUN apt-get install -y nodejs
RUN apt-get install typescript

COPY package.json /app/package.json
COPY package-lock.json /app/package-lock.json

RUN npm install
RUN tsc -p . # for building


FROM node as runner 
# now this second stage, copies all the code from the last step in the app/
# and then run that

WORKDIR app/

copy --from=build app/ . #from specifies the previous stage build
ENTRYPOINT [ "node", "index.js" ]


now all the packages of the typescript didint came to the last stage and the final image formed will be of the last stage
this is known as multi stage builds


